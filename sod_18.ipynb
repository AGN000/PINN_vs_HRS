{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is cusotmosed based on the root code provided in\n",
    "https://www.researchgate.net/publication/359480166_Discontinuity_Computing_using_Physics-Informed_Neural_Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import scipy.io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Seeds\n",
    "torch.manual_seed(12)\n",
    "np.random.seed(12)\n",
    "x_min=0;\n",
    "x_max=1;\n",
    "t_max=0.15\n",
    "nx=101;\n",
    "nt=101\n",
    "ct=1\n",
    "er_c1=1e-3\n",
    "# er_c2=1e-4\n",
    "   \n",
    "# Calculate gradients using torch.autograd.grad\n",
    "def gradients(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs,grad_outputs=torch.ones_like(outputs), create_graph=True)\n",
    "\n",
    "# Convert torch tensor into np.array\n",
    "def to_numpy(input):\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        return input.detach().cpu().numpy()\n",
    "    elif isinstance(input, np.ndarray):\n",
    "        return input\n",
    "    else:\n",
    "        raise TypeError('Unknown type of input, expected torch.Tensor or ' \\\n",
    "                        'np.ndarray, but got {}'.format(type(input)))\n",
    "\n",
    "# Initial conditions\n",
    "def IC(x):\n",
    "    N = len(x)\n",
    "    rho_init = np.zeros((x.shape[0]))                                              \n",
    "    u_init = np.zeros((x.shape[0]))                                                \n",
    "    p_init = np.zeros((x.shape[0]))                                                \n",
    "\n",
    "    # rho, p - initial condition\n",
    "    for i in range(N):\n",
    "        if (x[i] <= 0.5):\n",
    "            rho_init[i] = 1.0\n",
    "            p_init[i] = 1.0\n",
    "        else:\n",
    "            rho_init[i] = 0.125\n",
    "            p_init[i] = 0.1\n",
    "\n",
    "    return rho_init, u_init, p_init\n",
    "\n",
    "# Generate Neural Network adative tanh(ax) activation function\n",
    "class ParamTanh(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(ParamTanh, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.tensor(alpha, requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(self.alpha * x)\n",
    "class DNN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.net = nn.Sequential()    \n",
    "        tn=15                                              \n",
    "        self.net.add_module('Linear_layer_1', nn.Linear(2, tn))                     \n",
    "        self.net.add_module('Tanh_layer_1', nn.Tanh())                              \n",
    "\n",
    "        for num in range(2, 5):                                                     \n",
    "            self.net.add_module('Linear_layer_%d' % (num), nn.Linear(tn, tn))       \n",
    "            self.net.add_module('Tanh_layer_%d' % (num), ParamTanh(alpha=0.9))                 \n",
    "        self.net.add_module('Linear_layer_final', nn.Linear(tn, 4))                 \n",
    "\n",
    "    # Forward Feed\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "    # Loss function for PDE\n",
    "    def loss_pde(self, x):\n",
    "        y = self.net(x)                                                \n",
    "        rho,p,u,nui = y[:, 0:1], y[:, 1:2], y[:, 2:3], y[:, 3:4]\n",
    "        \n",
    "        U2 = rho*u\n",
    "        U3 = 0.5*rho*u**2 + p/0.4\n",
    "        \n",
    "        #F1 = U2\n",
    "        F2 = rho*u**2+p\n",
    "        F3 = u*(U3 + p)\n",
    "        \n",
    "        gamma = 1.4                                                    \n",
    "\n",
    "        # Gradients and partial derivatives\n",
    "        drho_g = gradients(rho, x)[0]                                  \n",
    "        rho_t, rho_x = drho_g[:, :1], drho_g[:, 1:]         \n",
    "\n",
    "        drho_gg = gradients(rho_x, x)[0]      \n",
    "        rho_tx, rho_xx = drho_gg[:, 0], drho_gg[:, 1]  \n",
    "\n",
    "\n",
    "        du_g = gradients(u, x)[0]                                      \n",
    "        u_t, u_x = du_g[:, :1], du_g[:, 1:]                            \n",
    "\n",
    "       # dp_g = gradients(p, x)[0]                                     \n",
    "       # p_t, p_x = dp_g[:, :1], dp_g[:, 1:]                           \n",
    "        \n",
    "        dU2_g = gradients(U2, x)[0]\n",
    "        U2_t,U2_x = dU2_g[:,:1], dU2_g[:,1:]\n",
    "        d2U2_g = gradients(U2_x, x)[0]\n",
    "        U2_tx,U2_xx = d2U2_g[:,:1], d2U2_g[:,1:]\n",
    "        dU3_g = gradients(U3, x)[0]\n",
    "        U3_t,U3_x = dU3_g[:,:1], dU3_g[:,1:]\n",
    "        d2U3_g = gradients(U3_x, x)[0]\n",
    "        U3_tx,U3_xx = d2U3_g[:,:1], d2U3_g[:,1:]\n",
    "        dF2_g = gradients(F2, x)[0]\n",
    "        F2_t,F2_x = dF2_g[:,:1], dF2_g[:,1:]\n",
    "        dF3_g = gradients(F3, x)[0]\n",
    "        F3_t,F3_x = dF3_g[:,:1], dF3_g[:,1:]\n",
    "\n",
    "        d = 0.12*(abs(u_x)-u_x)  + 1 #+0.1*(abs(rho_x)-rho_x)\n",
    "        #d = 0.1*(abs(u_x))+1e-18\n",
    "        # d=1;\n",
    "        nu=nui**2\n",
    "     \n",
    "        f = (((rho_t + U2_x-nu*rho_xx)/d)**2).mean() + \\\n",
    "            (((U2_t  + F2_x-nu*U2_xx)/d)**2).mean() + \\\n",
    "            (((U3_t  + F3_x-nu*U3_xx)/d)**2).mean() +((nu)**2).mean()\n",
    "           # ((rho_t).mean())**2  +\\\n",
    "           # ((U3_t).mean())**2 \n",
    "    \n",
    "        return f\n",
    "\n",
    "    # Loss function for initial condition\n",
    "    def loss_ic(self, x_ic, rho_ic, u_ic, p_ic):\n",
    "        y_ic = self.net(x_ic)                                                      \n",
    "        rho_ic_nn, p_ic_nn,u_ic_nn = y_ic[:, 0], y_ic[:, 1], y_ic[:, 2]            \n",
    "\n",
    "        # Loss function for the initial condition\n",
    "        loss_ics = ((u_ic_nn - u_ic) ** 2).mean() + \\\n",
    "               ((rho_ic_nn- rho_ic) ** 2).mean()  + \\\n",
    "               ((p_ic_nn - p_ic) ** 2).mean()\n",
    "\n",
    "        return loss_ics\n",
    "    \n",
    "#device = torch.device('cuda')         # change to cpu if you dont have a cuda device     \n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  \n",
    "# os.remove('loss.dat')                       \n",
    "                                                \n",
    "                                             \n",
    "x = np.linspace(x_min, x_max, nx)                                   \n",
    "t = np.linspace(0, t_max, nt)                                     \n",
    "t_grid, x_grid = np.meshgrid(t, x)                                 \n",
    "T = t_grid.flatten()[:, None]                                      \n",
    "X = x_grid.flatten()[:, None]                                      \n",
    "#id_f = np.random.choice(num_x*num_t, num_f_train, replace=False)   \n",
    "# we use all llocation points because PINN is free from over fitting. If you wish you can use random\n",
    "x_int = X[:, 0][:, None]                                        \n",
    "t_int = T[:, 0][:, None]                                        \n",
    "x_int_train = np.hstack((t_int, x_int))   \n",
    "\n",
    "x = np.linspace(x_min, x_max, nx)                                   \n",
    "t = np.linspace(0, t_max, nt)                                       \n",
    "t_grid, x_grid = np.meshgrid(t, x)                                 \n",
    "T = t_grid.flatten()[:, None]                                      \n",
    "X = x_grid.flatten()[:, None]                                      \n",
    "x_ic = x_grid[:, 0][:, None]                                   \n",
    "t_ic = t_grid[:, 0][:, None]                                   \n",
    "x_ic_train = np.hstack((t_ic, x_ic))                               \n",
    "\n",
    "rho_ic_train, u_ic_train, p_ic_train = IC(x_ic)                    \n",
    "x_ic_train = torch.tensor(x_ic_train, dtype=torch.float32).to(device)\n",
    "x_int_train = torch.tensor(x_int_train, requires_grad=True, dtype=torch.float32).to(device)\n",
    "# x_int_train1 = torch.tensor(x_int_train1, requires_grad=True, dtype=torch.float32).to(device)\n",
    "rho_ic_train = torch.tensor(rho_ic_train, dtype=torch.float32).to(device)\n",
    "u_ic_train = torch.tensor(u_ic_train, dtype=torch.float32).to(device)\n",
    "p_ic_train = torch.tensor(p_ic_train, dtype=torch.float32).to(device)\n",
    "\n",
    "model = DNN().to(device)\n",
    "\n",
    "def closure():\n",
    "        optimizer.zero_grad()                                                     \n",
    "        loss_pde = model.loss_pde(x_int_train)                                    \n",
    "        loss_ic = model.loss_ic(x_ic_train, rho_ic_train,u_ic_train,p_ic_train)   \n",
    "        loss = loss_pde + 1*loss_ic                                          \n",
    "        loss.backward()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: loss 1.197937\n",
      "epoch 2: loss 1.171996\n",
      "epoch 3: loss 1.146479\n",
      "epoch 4: loss 1.121322\n",
      "epoch 5: loss 1.096472\n",
      "epoch 6: loss 1.071912\n",
      "epoch 7: loss 1.047633\n",
      "epoch 8: loss 1.023627\n",
      "epoch 9: loss 0.999884\n",
      "epoch 10: loss 0.976390\n",
      "epoch 11: loss 0.953132\n",
      "epoch 12: loss 0.930094\n",
      "epoch 13: loss 0.907265\n",
      "epoch 14: loss 0.884633\n",
      "epoch 15: loss 0.862190\n",
      "epoch 16: loss 0.839933\n",
      "epoch 17: loss 0.817862\n",
      "epoch 18: loss 0.795983\n",
      "epoch 19: loss 0.774307\n",
      "epoch 20: loss 0.752848\n",
      "epoch 21: loss 0.731628\n",
      "epoch 22: loss 0.710671\n",
      "epoch 23: loss 0.690010\n",
      "epoch 24: loss 0.669682\n",
      "epoch 25: loss 0.649728\n",
      "epoch 26: loss 0.630195\n",
      "epoch 27: loss 0.611135\n",
      "epoch 28: loss 0.592603\n",
      "epoch 29: loss 0.574658\n",
      "epoch 30: loss 0.557361\n",
      "epoch 31: loss 0.540776\n",
      "epoch 32: loss 0.524966\n",
      "epoch 33: loss 0.509991\n",
      "epoch 34: loss 0.495911\n",
      "epoch 35: loss 0.482780\n",
      "epoch 36: loss 0.470647\n",
      "epoch 37: loss 0.459548\n",
      "epoch 38: loss 0.449511\n",
      "epoch 39: loss 0.440554\n",
      "epoch 40: loss 0.432676\n",
      "epoch 41: loss 0.425865\n",
      "epoch 42: loss 0.420092\n",
      "epoch 43: loss 0.415308\n",
      "epoch 44: loss 0.411450\n",
      "epoch 45: loss 0.408441\n",
      "epoch 46: loss 0.406186\n",
      "epoch 47: loss 0.404584\n",
      "epoch 48: loss 0.403521\n",
      "epoch 49: loss 0.402883\n",
      "epoch 50: loss 0.402555\n",
      "epoch 51: loss 0.402426\n",
      "epoch 52: loss 0.402394\n",
      "epoch 53: loss 0.402371\n",
      "epoch 54: loss 0.402283\n",
      "epoch 55: loss 0.402076\n",
      "epoch 56: loss 0.401712\n",
      "epoch 57: loss 0.401174\n",
      "epoch 58: loss 0.400457\n",
      "epoch 59: loss 0.399575\n",
      "epoch 60: loss 0.398548\n",
      "epoch 61: loss 0.397404\n",
      "epoch 62: loss 0.396177\n",
      "epoch 63: loss 0.394900\n",
      "epoch 64: loss 0.393605\n",
      "epoch 65: loss 0.392322\n",
      "epoch 66: loss 0.391074\n",
      "epoch 67: loss 0.389880\n",
      "epoch 68: loss 0.388754\n",
      "epoch 69: loss 0.387705\n",
      "epoch 70: loss 0.386734\n",
      "epoch 71: loss 0.385841\n",
      "epoch 72: loss 0.385020\n",
      "epoch 73: loss 0.384265\n",
      "epoch 74: loss 0.383565\n",
      "epoch 75: loss 0.382910\n",
      "epoch 76: loss 0.382290\n",
      "epoch 77: loss 0.381693\n",
      "epoch 78: loss 0.381110\n",
      "epoch 79: loss 0.380531\n",
      "epoch 80: loss 0.379950\n",
      "epoch 81: loss 0.379360\n",
      "epoch 82: loss 0.378756\n",
      "epoch 83: loss 0.378134\n",
      "epoch 84: loss 0.377494\n",
      "epoch 85: loss 0.376834\n",
      "epoch 86: loss 0.376154\n",
      "epoch 87: loss 0.375455\n",
      "epoch 88: loss 0.374739\n",
      "epoch 89: loss 0.374007\n",
      "epoch 90: loss 0.373260\n",
      "epoch 91: loss 0.372502\n",
      "epoch 92: loss 0.371733\n",
      "epoch 93: loss 0.370956\n",
      "epoch 94: loss 0.370170\n",
      "epoch 95: loss 0.369377\n",
      "epoch 96: loss 0.368576\n",
      "epoch 97: loss 0.367767\n",
      "epoch 98: loss 0.366950\n",
      "epoch 99: loss 0.366123\n",
      "epoch 100: loss 0.365285\n",
      "epoch 101: loss 0.364435\n",
      "epoch 102: loss 0.363571\n",
      "epoch 103: loss 0.362691\n",
      "epoch 104: loss 0.361794\n",
      "epoch 105: loss 0.360878\n",
      "epoch 106: loss 0.359943\n",
      "epoch 107: loss 0.358986\n",
      "epoch 108: loss 0.358007\n",
      "epoch 109: loss 0.357006\n",
      "epoch 110: loss 0.355982\n",
      "epoch 111: loss 0.354933\n",
      "epoch 112: loss 0.353860\n",
      "epoch 113: loss 0.352762\n",
      "epoch 114: loss 0.351639\n",
      "epoch 115: loss 0.350491\n",
      "epoch 116: loss 0.349316\n",
      "epoch 117: loss 0.348115\n",
      "epoch 118: loss 0.346887\n",
      "epoch 119: loss 0.345631\n",
      "epoch 120: loss 0.344347\n",
      "epoch 121: loss 0.343034\n",
      "epoch 122: loss 0.341691\n",
      "epoch 123: loss 0.340318\n",
      "epoch 124: loss 0.338913\n",
      "epoch 125: loss 0.337477\n",
      "epoch 126: loss 0.336008\n",
      "epoch 127: loss 0.334505\n",
      "epoch 128: loss 0.332969\n",
      "epoch 129: loss 0.331398\n",
      "epoch 130: loss 0.329792\n",
      "epoch 131: loss 0.328152\n",
      "epoch 132: loss 0.326475\n",
      "epoch 133: loss 0.324763\n",
      "epoch 134: loss 0.323015\n",
      "epoch 135: loss 0.321231\n",
      "epoch 136: loss 0.319410\n",
      "epoch 137: loss 0.317553\n",
      "epoch 138: loss 0.315660\n",
      "epoch 139: loss 0.313730\n",
      "epoch 140: loss 0.311765\n",
      "epoch 141: loss 0.309763\n",
      "epoch 142: loss 0.307726\n",
      "epoch 143: loss 0.305655\n",
      "epoch 144: loss 0.303549\n",
      "epoch 145: loss 0.301410\n",
      "epoch 146: loss 0.299241\n",
      "epoch 147: loss 0.297043\n",
      "epoch 148: loss 0.294819\n",
      "epoch 149: loss 0.292573\n",
      "epoch 150: loss 0.290310\n",
      "epoch 151: loss 0.288035\n",
      "epoch 152: loss 0.285756\n",
      "epoch 153: loss 0.283480\n",
      "epoch 154: loss 0.281216\n",
      "epoch 155: loss 0.278974\n",
      "epoch 156: loss 0.276763\n",
      "epoch 157: loss 0.274596\n",
      "epoch 158: loss 0.272481\n",
      "epoch 159: loss 0.270430\n",
      "epoch 160: loss 0.268452\n",
      "epoch 161: loss 0.266556\n",
      "epoch 162: loss 0.264748\n",
      "epoch 163: loss 0.263034\n",
      "epoch 164: loss 0.261418\n",
      "epoch 165: loss 0.259902\n",
      "epoch 166: loss 0.258487\n",
      "epoch 167: loss 0.257173\n",
      "epoch 168: loss 0.255959\n",
      "epoch 169: loss 0.254843\n",
      "epoch 170: loss 0.253824\n",
      "epoch 171: loss 0.252896\n",
      "epoch 172: loss 0.252054\n",
      "epoch 173: loss 0.251291\n",
      "epoch 174: loss 0.250597\n",
      "epoch 175: loss 0.249961\n",
      "epoch 176: loss 0.249372\n",
      "epoch 177: loss 0.248818\n",
      "epoch 178: loss 0.248290\n",
      "epoch 179: loss 0.247778\n",
      "epoch 180: loss 0.247274\n",
      "epoch 181: loss 0.246774\n",
      "epoch 182: loss 0.246272\n",
      "epoch 183: loss 0.245766\n",
      "epoch 184: loss 0.245252\n",
      "epoch 185: loss 0.244730\n",
      "epoch 186: loss 0.244199\n",
      "epoch 187: loss 0.243660\n",
      "epoch 188: loss 0.243114\n",
      "epoch 189: loss 0.242565\n",
      "epoch 190: loss 0.242014\n",
      "epoch 191: loss 0.241465\n",
      "epoch 192: loss 0.240919\n",
      "epoch 193: loss 0.240377\n",
      "epoch 194: loss 0.239841\n",
      "epoch 195: loss 0.239312\n",
      "epoch 196: loss 0.238790\n",
      "epoch 197: loss 0.238275\n",
      "epoch 198: loss 0.237769\n",
      "epoch 199: loss 0.237270\n",
      "epoch 200: loss 0.236778\n",
      "epoch 201: loss 0.236294\n",
      "epoch 202: loss 0.235815\n",
      "epoch 203: loss 0.235342\n",
      "epoch 204: loss 0.234874\n",
      "epoch 205: loss 0.234409\n",
      "epoch 206: loss 0.233948\n",
      "epoch 207: loss 0.233488\n",
      "epoch 208: loss 0.233031\n",
      "epoch 209: loss 0.232574\n",
      "epoch 210: loss 0.232117\n",
      "epoch 211: loss 0.231661\n",
      "epoch 212: loss 0.231203\n",
      "epoch 213: loss 0.230745\n",
      "epoch 214: loss 0.230286\n",
      "epoch 215: loss 0.229825\n",
      "epoch 216: loss 0.229362\n",
      "epoch 217: loss 0.228898\n",
      "epoch 218: loss 0.228433\n",
      "epoch 219: loss 0.227965\n",
      "epoch 220: loss 0.227495\n",
      "epoch 221: loss 0.227023\n",
      "epoch 222: loss 0.226549\n",
      "epoch 223: loss 0.226072\n",
      "epoch 224: loss 0.225593\n",
      "epoch 225: loss 0.225111\n",
      "epoch 226: loss 0.224627\n",
      "epoch 227: loss 0.224140\n",
      "epoch 228: loss 0.223649\n",
      "epoch 229: loss 0.223155\n",
      "epoch 230: loss 0.222658\n",
      "epoch 231: loss 0.222157\n",
      "epoch 232: loss 0.221652\n",
      "epoch 233: loss 0.221143\n",
      "epoch 234: loss 0.220630\n",
      "epoch 235: loss 0.220112\n",
      "epoch 236: loss 0.219590\n",
      "epoch 237: loss 0.219063\n",
      "epoch 238: loss 0.218531\n",
      "epoch 239: loss 0.217994\n",
      "epoch 240: loss 0.217451\n",
      "epoch 241: loss 0.216903\n",
      "epoch 242: loss 0.216349\n",
      "epoch 243: loss 0.215789\n",
      "epoch 244: loss 0.215223\n",
      "epoch 245: loss 0.214652\n",
      "epoch 246: loss 0.214073\n",
      "epoch 247: loss 0.213489\n",
      "epoch 248: loss 0.212898\n",
      "epoch 249: loss 0.212300\n",
      "epoch 250: loss 0.211695\n",
      "epoch 251: loss 0.211083\n",
      "epoch 252: loss 0.210464\n",
      "epoch 253: loss 0.209837\n",
      "epoch 254: loss 0.209203\n",
      "epoch 255: loss 0.208562\n",
      "epoch 256: loss 0.207912\n",
      "epoch 257: loss 0.207254\n",
      "epoch 258: loss 0.206588\n",
      "epoch 259: loss 0.205913\n",
      "epoch 260: loss 0.205229\n",
      "epoch 261: loss 0.204536\n",
      "epoch 262: loss 0.203834\n",
      "epoch 263: loss 0.203123\n",
      "epoch 264: loss 0.202401\n",
      "epoch 265: loss 0.201669\n",
      "epoch 266: loss 0.200927\n",
      "epoch 267: loss 0.200173\n",
      "epoch 268: loss 0.199409\n",
      "epoch 269: loss 0.198633\n",
      "epoch 270: loss 0.197845\n",
      "epoch 271: loss 0.197045\n",
      "epoch 272: loss 0.196232\n",
      "epoch 273: loss 0.195406\n",
      "epoch 274: loss 0.194566\n",
      "epoch 275: loss 0.193713\n",
      "epoch 276: loss 0.192845\n",
      "epoch 277: loss 0.191963\n",
      "epoch 278: loss 0.191065\n",
      "epoch 279: loss 0.190152\n",
      "epoch 280: loss 0.189223\n",
      "epoch 281: loss 0.188277\n",
      "epoch 282: loss 0.187315\n",
      "epoch 283: loss 0.186336\n",
      "epoch 284: loss 0.185339\n",
      "epoch 285: loss 0.184325\n",
      "epoch 286: loss 0.183292\n",
      "epoch 287: loss 0.182242\n",
      "epoch 288: loss 0.181173\n",
      "epoch 289: loss 0.180086\n",
      "epoch 290: loss 0.178981\n",
      "epoch 291: loss 0.177857\n",
      "epoch 292: loss 0.176715\n",
      "epoch 293: loss 0.175556\n",
      "epoch 294: loss 0.174379\n",
      "epoch 295: loss 0.173185\n",
      "epoch 296: loss 0.171975\n",
      "epoch 297: loss 0.170751\n",
      "epoch 298: loss 0.169511\n",
      "epoch 299: loss 0.168259\n",
      "epoch 300: loss 0.166996\n",
      "epoch 301: loss 0.165722\n",
      "epoch 302: loss 0.164440\n",
      "epoch 303: loss 0.163153\n",
      "epoch 304: loss 0.161861\n",
      "epoch 305: loss 0.160568\n",
      "epoch 306: loss 0.159275\n",
      "epoch 307: loss 0.157987\n",
      "epoch 308: loss 0.156705\n",
      "epoch 309: loss 0.155433\n",
      "epoch 310: loss 0.154173\n",
      "epoch 311: loss 0.152929\n",
      "epoch 312: loss 0.151704\n",
      "epoch 313: loss 0.150501\n",
      "epoch 314: loss 0.149322\n",
      "epoch 315: loss 0.148170\n",
      "epoch 316: loss 0.147048\n",
      "epoch 317: loss 0.145958\n",
      "epoch 318: loss 0.144901\n",
      "epoch 319: loss 0.143878\n",
      "epoch 320: loss 0.142889\n",
      "epoch 321: loss 0.141935\n",
      "epoch 322: loss 0.141015\n",
      "epoch 323: loss 0.140130\n",
      "epoch 324: loss 0.139279\n",
      "epoch 325: loss 0.138461\n",
      "epoch 326: loss 0.137676\n",
      "epoch 327: loss 0.136920\n",
      "epoch 328: loss 0.136196\n",
      "epoch 329: loss 0.135518\n",
      "epoch 330: loss 0.135040\n",
      "epoch 331: loss 0.135588\n",
      "epoch 332: loss 0.136576\n",
      "epoch 333: loss 0.133924\n",
      "epoch 334: loss 0.133409\n",
      "epoch 335: loss 0.134066\n",
      "epoch 336: loss 0.131836\n",
      "epoch 337: loss 0.132851\n",
      "epoch 338: loss 0.131371\n",
      "epoch 339: loss 0.131286\n",
      "epoch 340: loss 0.130967\n",
      "epoch 341: loss 0.129999\n",
      "epoch 342: loss 0.130322\n",
      "epoch 343: loss 0.129088\n",
      "epoch 344: loss 0.129454\n",
      "epoch 345: loss 0.128428\n",
      "epoch 346: loss 0.128481\n",
      "epoch 347: loss 0.127862\n",
      "epoch 348: loss 0.127532\n",
      "epoch 349: loss 0.127285\n",
      "epoch 350: loss 0.126690\n",
      "epoch 351: loss 0.126649\n",
      "epoch 352: loss 0.125976\n",
      "epoch 353: loss 0.125949\n",
      "epoch 354: loss 0.125367\n",
      "epoch 355: loss 0.125222\n",
      "epoch 356: loss 0.124815\n",
      "epoch 357: loss 0.124513\n",
      "epoch 358: loss 0.124270\n",
      "epoch 359: loss 0.123861\n",
      "epoch 360: loss 0.123703\n",
      "epoch 361: loss 0.123278\n",
      "epoch 362: loss 0.123112\n",
      "epoch 363: loss 0.122751\n",
      "epoch 364: loss 0.122516\n",
      "epoch 365: loss 0.122249\n",
      "epoch 366: loss 0.121945\n",
      "epoch 367: loss 0.121743\n",
      "epoch 368: loss 0.121418\n",
      "epoch 369: loss 0.121222\n",
      "epoch 370: loss 0.120931\n",
      "epoch 371: loss 0.120698\n",
      "epoch 372: loss 0.120463\n",
      "epoch 373: loss 0.120195\n",
      "epoch 374: loss 0.119992\n",
      "epoch 375: loss 0.119724\n",
      "epoch 376: loss 0.119514\n",
      "epoch 377: loss 0.119278\n",
      "epoch 378: loss 0.119043\n",
      "epoch 379: loss 0.118837\n",
      "epoch 380: loss 0.118595\n",
      "epoch 381: loss 0.118391\n",
      "epoch 382: loss 0.118169\n",
      "epoch 383: loss 0.117950\n",
      "epoch 384: loss 0.117749\n",
      "epoch 385: loss 0.117526\n",
      "epoch 386: loss 0.117326\n",
      "epoch 387: loss 0.117119\n",
      "epoch 388: loss 0.116909\n",
      "epoch 389: loss 0.116714\n",
      "epoch 390: loss 0.116507\n",
      "epoch 391: loss 0.116309\n",
      "epoch 392: loss 0.116115\n",
      "epoch 393: loss 0.115913\n",
      "epoch 394: loss 0.115723\n",
      "epoch 395: loss 0.115530\n",
      "epoch 396: loss 0.115335\n",
      "epoch 397: loss 0.115149\n",
      "epoch 398: loss 0.114958\n",
      "epoch 399: loss 0.114769\n",
      "epoch 400: loss 0.114586\n",
      "epoch 401: loss 0.114399\n",
      "epoch 402: loss 0.114215\n",
      "epoch 403: loss 0.114034\n",
      "epoch 404: loss 0.113850\n",
      "epoch 405: loss 0.113670\n",
      "epoch 406: loss 0.113492\n",
      "epoch 407: loss 0.113312\n",
      "epoch 408: loss 0.113135\n",
      "epoch 409: loss 0.112960\n",
      "epoch 410: loss 0.112783\n",
      "epoch 411: loss 0.112609\n",
      "epoch 412: loss 0.112436\n",
      "epoch 413: loss 0.112263\n",
      "epoch 414: loss 0.112091\n",
      "epoch 415: loss 0.111921\n",
      "epoch 416: loss 0.111751\n",
      "epoch 417: loss 0.111582\n",
      "epoch 418: loss 0.111414\n",
      "epoch 419: loss 0.111247\n",
      "epoch 420: loss 0.111080\n",
      "epoch 421: loss 0.110915\n",
      "epoch 422: loss 0.110751\n",
      "epoch 423: loss 0.110586\n",
      "epoch 424: loss 0.110423\n",
      "epoch 425: loss 0.110261\n",
      "epoch 426: loss 0.110100\n",
      "epoch 427: loss 0.109939\n",
      "epoch 428: loss 0.109780\n",
      "epoch 429: loss 0.109621\n",
      "epoch 430: loss 0.109463\n",
      "epoch 431: loss 0.109305\n",
      "epoch 432: loss 0.109149\n",
      "epoch 433: loss 0.108993\n",
      "epoch 434: loss 0.108838\n",
      "epoch 435: loss 0.108683\n",
      "epoch 436: loss 0.108530\n",
      "epoch 437: loss 0.108377\n",
      "epoch 438: loss 0.108225\n",
      "epoch 439: loss 0.108074\n",
      "epoch 440: loss 0.107923\n",
      "epoch 441: loss 0.107773\n",
      "epoch 442: loss 0.107624\n",
      "epoch 443: loss 0.107476\n",
      "epoch 444: loss 0.107328\n",
      "epoch 445: loss 0.107181\n",
      "epoch 446: loss 0.107035\n",
      "epoch 447: loss 0.106889\n",
      "epoch 448: loss 0.106744\n",
      "epoch 449: loss 0.106600\n",
      "epoch 450: loss 0.106456\n",
      "epoch 451: loss 0.106313\n",
      "epoch 452: loss 0.106170\n",
      "epoch 453: loss 0.106028\n",
      "epoch 454: loss 0.105886\n",
      "epoch 455: loss 0.105745\n",
      "epoch 456: loss 0.105605\n",
      "epoch 457: loss 0.105465\n",
      "epoch 458: loss 0.105325\n",
      "epoch 459: loss 0.105186\n",
      "epoch 460: loss 0.105047\n",
      "epoch 461: loss 0.104909\n",
      "epoch 462: loss 0.104771\n",
      "epoch 463: loss 0.104633\n",
      "epoch 464: loss 0.104496\n",
      "epoch 465: loss 0.104359\n",
      "epoch 466: loss 0.104222\n",
      "epoch 467: loss 0.104086\n",
      "epoch 468: loss 0.103949\n",
      "epoch 469: loss 0.103813\n",
      "epoch 470: loss 0.103677\n",
      "epoch 471: loss 0.103541\n",
      "epoch 472: loss 0.103405\n",
      "epoch 473: loss 0.103270\n",
      "epoch 474: loss 0.103134\n",
      "epoch 475: loss 0.102998\n",
      "epoch 476: loss 0.102863\n",
      "epoch 477: loss 0.102727\n",
      "epoch 478: loss 0.102591\n",
      "epoch 479: loss 0.102456\n",
      "epoch 480: loss 0.102320\n",
      "epoch 481: loss 0.102184\n",
      "epoch 482: loss 0.102048\n",
      "epoch 483: loss 0.101911\n",
      "epoch 484: loss 0.101775\n",
      "epoch 485: loss 0.101638\n",
      "epoch 486: loss 0.101501\n",
      "epoch 487: loss 0.101364\n",
      "epoch 488: loss 0.101227\n",
      "epoch 489: loss 0.101089\n",
      "epoch 490: loss 0.100953\n",
      "epoch 491: loss 0.100819\n",
      "epoch 492: loss 0.100691\n",
      "epoch 493: loss 0.100579\n",
      "epoch 494: loss 0.100504\n",
      "epoch 495: loss 0.100520\n",
      "epoch 496: loss 0.100702\n",
      "epoch 497: loss 0.101106\n",
      "epoch 498: loss 0.101381\n",
      "epoch 499: loss 0.100939\n",
      "epoch 500: loss 0.099882\n",
      "epoch 501: loss 0.099513\n",
      "epoch 502: loss 0.099973\n",
      "epoch 503: loss 0.100025\n",
      "epoch 504: loss 0.099332\n",
      "epoch 505: loss 0.098990\n",
      "epoch 506: loss 0.099279\n",
      "epoch 507: loss 0.099204\n",
      "epoch 508: loss 0.098665\n",
      "epoch 509: loss 0.098545\n",
      "epoch 510: loss 0.098706\n",
      "epoch 511: loss 0.098446\n",
      "epoch 512: loss 0.098090\n",
      "epoch 513: loss 0.098110\n",
      "epoch 514: loss 0.098089\n",
      "epoch 515: loss 0.097781\n",
      "epoch 516: loss 0.097604\n",
      "epoch 517: loss 0.097620\n",
      "epoch 518: loss 0.097465\n",
      "epoch 519: loss 0.097211\n",
      "epoch 520: loss 0.097133\n",
      "epoch 521: loss 0.097076\n",
      "epoch 522: loss 0.096870\n",
      "epoch 523: loss 0.096695\n",
      "epoch 524: loss 0.096632\n",
      "epoch 525: loss 0.096510\n",
      "epoch 526: loss 0.096315\n",
      "epoch 527: loss 0.096188\n",
      "epoch 528: loss 0.096101\n",
      "epoch 529: loss 0.095949\n",
      "epoch 530: loss 0.095780\n",
      "epoch 531: loss 0.095667\n",
      "epoch 532: loss 0.095556\n",
      "epoch 533: loss 0.095398\n",
      "epoch 534: loss 0.095246\n",
      "epoch 535: loss 0.095131\n",
      "epoch 536: loss 0.095004\n",
      "epoch 537: loss 0.094849\n",
      "epoch 538: loss 0.094706\n",
      "epoch 539: loss 0.094583\n",
      "epoch 540: loss 0.094449\n",
      "epoch 541: loss 0.094297\n",
      "epoch 542: loss 0.094155\n",
      "epoch 543: loss 0.094026\n",
      "epoch 544: loss 0.093888\n",
      "epoch 545: loss 0.093738\n",
      "epoch 546: loss 0.093595\n",
      "epoch 547: loss 0.093460\n",
      "epoch 548: loss 0.093320\n",
      "epoch 549: loss 0.093172\n",
      "epoch 550: loss 0.093026\n",
      "epoch 551: loss 0.092886\n",
      "epoch 552: loss 0.092744\n",
      "epoch 553: loss 0.092596\n",
      "epoch 554: loss 0.092448\n",
      "epoch 555: loss 0.092303\n",
      "epoch 556: loss 0.092159\n",
      "epoch 557: loss 0.092011\n",
      "epoch 558: loss 0.091861\n",
      "epoch 559: loss 0.091713\n",
      "epoch 560: loss 0.091565\n",
      "epoch 561: loss 0.091417\n",
      "epoch 562: loss 0.091265\n",
      "epoch 563: loss 0.091114\n",
      "epoch 564: loss 0.090963\n",
      "epoch 565: loss 0.090812\n",
      "epoch 566: loss 0.090660\n",
      "epoch 567: loss 0.090506\n",
      "epoch 568: loss 0.090353\n",
      "epoch 569: loss 0.090199\n",
      "epoch 570: loss 0.090046\n",
      "epoch 571: loss 0.089891\n",
      "epoch 572: loss 0.089735\n",
      "epoch 573: loss 0.089579\n",
      "epoch 574: loss 0.089422\n",
      "epoch 575: loss 0.089266\n",
      "epoch 576: loss 0.089109\n",
      "epoch 577: loss 0.088950\n",
      "epoch 578: loss 0.088792\n",
      "epoch 579: loss 0.088633\n",
      "epoch 580: loss 0.088474\n",
      "epoch 581: loss 0.088314\n",
      "epoch 582: loss 0.088154\n",
      "epoch 583: loss 0.087994\n",
      "epoch 584: loss 0.087832\n",
      "epoch 585: loss 0.087671\n",
      "epoch 586: loss 0.087509\n",
      "epoch 587: loss 0.087347\n",
      "epoch 588: loss 0.087185\n",
      "epoch 589: loss 0.087022\n",
      "epoch 590: loss 0.086858\n",
      "epoch 591: loss 0.086694\n",
      "epoch 592: loss 0.086530\n",
      "epoch 593: loss 0.086366\n",
      "epoch 594: loss 0.086201\n",
      "epoch 595: loss 0.086036\n",
      "epoch 596: loss 0.085871\n",
      "epoch 597: loss 0.085705\n",
      "epoch 598: loss 0.085539\n",
      "epoch 599: loss 0.085373\n",
      "epoch 600: loss 0.085206\n",
      "epoch 601: loss 0.085039\n",
      "epoch 602: loss 0.084872\n",
      "epoch 603: loss 0.084704\n",
      "epoch 604: loss 0.084537\n",
      "epoch 605: loss 0.084369\n",
      "epoch 606: loss 0.084201\n",
      "epoch 607: loss 0.084032\n",
      "epoch 608: loss 0.083864\n",
      "epoch 609: loss 0.083695\n",
      "epoch 610: loss 0.083526\n",
      "epoch 611: loss 0.083357\n",
      "epoch 612: loss 0.083187\n",
      "epoch 613: loss 0.083018\n",
      "epoch 614: loss 0.082848\n",
      "epoch 615: loss 0.082678\n",
      "epoch 616: loss 0.082508\n",
      "epoch 617: loss 0.082338\n",
      "epoch 618: loss 0.082169\n",
      "epoch 619: loss 0.081999\n",
      "epoch 620: loss 0.081831\n",
      "epoch 621: loss 0.081666\n",
      "epoch 622: loss 0.081505\n",
      "epoch 623: loss 0.081355\n",
      "epoch 624: loss 0.081227\n",
      "epoch 625: loss 0.081145\n",
      "epoch 626: loss 0.081159\n",
      "epoch 627: loss 0.081336\n",
      "epoch 628: loss 0.081735\n",
      "epoch 629: loss 0.082150\n",
      "epoch 630: loss 0.082010\n",
      "epoch 631: loss 0.080920\n",
      "epoch 632: loss 0.079883\n",
      "epoch 633: loss 0.079919\n",
      "epoch 634: loss 0.080449\n",
      "epoch 635: loss 0.080264\n",
      "epoch 636: loss 0.079414\n",
      "epoch 637: loss 0.079093\n",
      "epoch 638: loss 0.079409\n",
      "epoch 639: loss 0.079354\n",
      "epoch 640: loss 0.078755\n",
      "epoch 641: loss 0.078458\n",
      "epoch 642: loss 0.078621\n",
      "epoch 643: loss 0.078532\n",
      "epoch 644: loss 0.078075\n",
      "epoch 645: loss 0.077850\n",
      "epoch 646: loss 0.077917\n",
      "epoch 647: loss 0.077781\n",
      "epoch 648: loss 0.077423\n",
      "epoch 649: loss 0.077243\n",
      "epoch 650: loss 0.077240\n",
      "epoch 651: loss 0.077088\n",
      "epoch 652: loss 0.076798\n",
      "epoch 653: loss 0.076633\n",
      "epoch 654: loss 0.076582\n",
      "epoch 655: loss 0.076431\n",
      "epoch 656: loss 0.076189\n",
      "epoch 657: loss 0.076023\n",
      "epoch 658: loss 0.075938\n",
      "epoch 659: loss 0.075798\n",
      "epoch 660: loss 0.075589\n",
      "epoch 661: loss 0.075417\n",
      "epoch 662: loss 0.075306\n",
      "epoch 663: loss 0.075177\n",
      "epoch 664: loss 0.074996\n",
      "epoch 665: loss 0.074819\n",
      "epoch 666: loss 0.074687\n",
      "epoch 667: loss 0.074562\n",
      "epoch 668: loss 0.074405\n",
      "epoch 669: loss 0.074233\n",
      "epoch 670: loss 0.074082\n",
      "epoch 671: loss 0.073951\n",
      "epoch 672: loss 0.073812\n",
      "epoch 673: loss 0.073654\n",
      "epoch 674: loss 0.073495\n",
      "epoch 675: loss 0.073351\n",
      "epoch 676: loss 0.073216\n",
      "epoch 677: loss 0.073075\n",
      "epoch 678: loss 0.072923\n",
      "epoch 679: loss 0.072771\n",
      "epoch 680: loss 0.072628\n",
      "epoch 681: loss 0.072491\n",
      "epoch 682: loss 0.072352\n",
      "epoch 683: loss 0.072207\n",
      "epoch 684: loss 0.072060\n",
      "epoch 685: loss 0.071917\n",
      "epoch 686: loss 0.071779\n",
      "epoch 687: loss 0.071642\n",
      "epoch 688: loss 0.071505\n",
      "epoch 689: loss 0.071365\n",
      "epoch 690: loss 0.071224\n",
      "epoch 691: loss 0.071085\n",
      "epoch 692: loss 0.070949\n",
      "epoch 693: loss 0.070815\n",
      "epoch 694: loss 0.070682\n",
      "epoch 695: loss 0.070548\n",
      "epoch 696: loss 0.070414\n",
      "epoch 697: loss 0.070280\n",
      "epoch 698: loss 0.070147\n",
      "epoch 699: loss 0.070016\n",
      "epoch 700: loss 0.069886\n",
      "epoch 701: loss 0.069757\n",
      "epoch 702: loss 0.069630\n",
      "epoch 703: loss 0.069503\n",
      "epoch 704: loss 0.069376\n",
      "epoch 705: loss 0.069251\n",
      "epoch 706: loss 0.069126\n",
      "epoch 707: loss 0.069001\n",
      "epoch 708: loss 0.068878\n",
      "epoch 709: loss 0.068756\n",
      "epoch 710: loss 0.068635\n",
      "epoch 711: loss 0.068514\n",
      "epoch 712: loss 0.068395\n",
      "epoch 713: loss 0.068277\n",
      "epoch 714: loss 0.068160\n",
      "epoch 715: loss 0.068044\n",
      "epoch 716: loss 0.067929\n",
      "epoch 717: loss 0.067816\n",
      "epoch 718: loss 0.067704\n",
      "epoch 719: loss 0.067593\n",
      "epoch 720: loss 0.067486\n",
      "epoch 721: loss 0.067383\n",
      "epoch 722: loss 0.067288\n",
      "epoch 723: loss 0.067210\n",
      "epoch 724: loss 0.067165\n",
      "epoch 725: loss 0.067189\n",
      "epoch 726: loss 0.067352\n",
      "epoch 727: loss 0.067773\n",
      "epoch 728: loss 0.068522\n",
      "epoch 729: loss 0.069367\n",
      "epoch 730: loss 0.069293\n",
      "epoch 731: loss 0.067802\n",
      "epoch 732: loss 0.066341\n",
      "epoch 733: loss 0.066587\n",
      "epoch 734: loss 0.067570\n",
      "epoch 735: loss 0.067325\n",
      "epoch 736: loss 0.066158\n",
      "epoch 737: loss 0.065940\n",
      "epoch 738: loss 0.066605\n",
      "epoch 739: loss 0.066531\n",
      "epoch 740: loss 0.065720\n",
      "epoch 741: loss 0.065592\n",
      "epoch 742: loss 0.066022\n",
      "epoch 743: loss 0.065847\n",
      "epoch 744: loss 0.065293\n",
      "epoch 745: loss 0.065287\n",
      "epoch 746: loss 0.065528\n",
      "epoch 747: loss 0.065297\n",
      "epoch 748: loss 0.064933\n",
      "epoch 749: loss 0.064982\n",
      "epoch 750: loss 0.065087\n",
      "epoch 751: loss 0.064849\n",
      "epoch 752: loss 0.064617\n",
      "epoch 753: loss 0.064664\n",
      "epoch 754: loss 0.064680\n",
      "epoch 755: loss 0.064476\n",
      "epoch 756: loss 0.064320\n",
      "epoch 757: loss 0.064341\n",
      "epoch 758: loss 0.064315\n",
      "epoch 759: loss 0.064149\n",
      "epoch 760: loss 0.064030\n",
      "epoch 761: loss 0.064024\n",
      "epoch 762: loss 0.063980\n",
      "epoch 763: loss 0.063848\n",
      "epoch 764: loss 0.063745\n",
      "epoch 765: loss 0.063717\n",
      "epoch 766: loss 0.063669\n",
      "epoch 767: loss 0.063561\n",
      "epoch 768: loss 0.063466\n",
      "epoch 769: loss 0.063421\n",
      "epoch 770: loss 0.063373\n",
      "epoch 771: loss 0.063285\n",
      "epoch 772: loss 0.063194\n",
      "epoch 773: loss 0.063137\n",
      "epoch 774: loss 0.063088\n",
      "epoch 775: loss 0.063014\n",
      "epoch 776: loss 0.062930\n",
      "epoch 777: loss 0.062864\n",
      "epoch 778: loss 0.062811\n",
      "epoch 779: loss 0.062748\n",
      "epoch 780: loss 0.062672\n",
      "epoch 781: loss 0.062601\n",
      "epoch 782: loss 0.062542\n",
      "epoch 783: loss 0.062485\n",
      "epoch 784: loss 0.062418\n",
      "epoch 785: loss 0.062347\n",
      "epoch 786: loss 0.062283\n",
      "epoch 787: loss 0.062225\n",
      "epoch 788: loss 0.062164\n",
      "epoch 789: loss 0.062099\n",
      "epoch 790: loss 0.062033\n",
      "epoch 791: loss 0.061971\n",
      "epoch 792: loss 0.061913\n",
      "epoch 793: loss 0.061852\n",
      "epoch 794: loss 0.061789\n",
      "epoch 795: loss 0.061726\n",
      "epoch 796: loss 0.061666\n",
      "epoch 797: loss 0.061607\n",
      "epoch 798: loss 0.061548\n",
      "epoch 799: loss 0.061487\n",
      "epoch 800: loss 0.061426\n",
      "epoch 801: loss 0.061366\n",
      "epoch 802: loss 0.061308\n",
      "epoch 803: loss 0.061250\n",
      "epoch 804: loss 0.061191\n",
      "epoch 805: loss 0.061132\n",
      "epoch 806: loss 0.061073\n",
      "epoch 807: loss 0.061015\n",
      "epoch 808: loss 0.060958\n",
      "epoch 809: loss 0.060900\n",
      "epoch 810: loss 0.060843\n",
      "epoch 811: loss 0.060785\n",
      "epoch 812: loss 0.060728\n",
      "epoch 813: loss 0.060671\n",
      "epoch 814: loss 0.060615\n",
      "epoch 815: loss 0.060559\n",
      "epoch 816: loss 0.060502\n",
      "epoch 817: loss 0.060446\n",
      "epoch 818: loss 0.060390\n",
      "epoch 819: loss 0.060335\n",
      "epoch 820: loss 0.060279\n",
      "epoch 821: loss 0.060224\n",
      "epoch 822: loss 0.060169\n",
      "epoch 823: loss 0.060114\n",
      "epoch 824: loss 0.060060\n",
      "epoch 825: loss 0.060005\n",
      "epoch 826: loss 0.059951\n",
      "epoch 827: loss 0.059897\n",
      "epoch 828: loss 0.059843\n",
      "epoch 829: loss 0.059789\n",
      "epoch 830: loss 0.059736\n",
      "epoch 831: loss 0.059682\n",
      "epoch 832: loss 0.059629\n",
      "epoch 833: loss 0.059576\n",
      "epoch 834: loss 0.059523\n",
      "epoch 835: loss 0.059471\n",
      "epoch 836: loss 0.059418\n",
      "epoch 837: loss 0.059366\n",
      "epoch 838: loss 0.059314\n",
      "epoch 839: loss 0.059262\n",
      "epoch 840: loss 0.059210\n",
      "epoch 841: loss 0.059158\n",
      "epoch 842: loss 0.059107\n",
      "epoch 843: loss 0.059055\n",
      "epoch 844: loss 0.059004\n",
      "epoch 845: loss 0.058953\n",
      "epoch 846: loss 0.058902\n",
      "epoch 847: loss 0.058852\n",
      "epoch 848: loss 0.058801\n",
      "epoch 849: loss 0.058751\n",
      "epoch 850: loss 0.058700\n",
      "epoch 851: loss 0.058650\n",
      "epoch 852: loss 0.058600\n",
      "epoch 853: loss 0.058551\n",
      "epoch 854: loss 0.058501\n",
      "epoch 855: loss 0.058452\n",
      "epoch 856: loss 0.058402\n",
      "epoch 857: loss 0.058353\n",
      "epoch 858: loss 0.058304\n",
      "epoch 859: loss 0.058255\n",
      "epoch 860: loss 0.058206\n",
      "epoch 861: loss 0.058158\n",
      "epoch 862: loss 0.058109\n",
      "epoch 863: loss 0.058061\n",
      "epoch 864: loss 0.058013\n",
      "epoch 865: loss 0.057965\n",
      "epoch 866: loss 0.057917\n",
      "epoch 867: loss 0.057869\n",
      "epoch 868: loss 0.057822\n",
      "epoch 869: loss 0.057774\n",
      "epoch 870: loss 0.057727\n",
      "epoch 871: loss 0.057680\n",
      "epoch 872: loss 0.057633\n",
      "epoch 873: loss 0.057586\n",
      "epoch 874: loss 0.057540\n",
      "epoch 875: loss 0.057494\n",
      "epoch 876: loss 0.057448\n",
      "epoch 877: loss 0.057404\n",
      "epoch 878: loss 0.057362\n",
      "epoch 879: loss 0.057323\n",
      "epoch 880: loss 0.057291\n",
      "epoch 881: loss 0.057271\n",
      "epoch 882: loss 0.057275\n",
      "epoch 883: loss 0.057323\n",
      "epoch 884: loss 0.057455\n",
      "epoch 885: loss 0.057728\n",
      "epoch 886: loss 0.058218\n",
      "epoch 887: loss 0.058897\n",
      "epoch 888: loss 0.059520\n",
      "epoch 889: loss 0.059378\n",
      "epoch 890: loss 0.058267\n",
      "epoch 891: loss 0.057015\n",
      "epoch 892: loss 0.056840\n",
      "epoch 893: loss 0.057576\n",
      "epoch 894: loss 0.057990\n",
      "epoch 895: loss 0.057445\n",
      "epoch 896: loss 0.056670\n",
      "epoch 897: loss 0.056678\n",
      "epoch 898: loss 0.057174\n",
      "epoch 899: loss 0.057194\n",
      "epoch 900: loss 0.056670\n",
      "epoch 901: loss 0.056383\n",
      "epoch 902: loss 0.056625\n",
      "epoch 903: loss 0.056808\n",
      "epoch 904: loss 0.056541\n",
      "epoch 905: loss 0.056236\n",
      "epoch 906: loss 0.056289\n",
      "epoch 907: loss 0.056454\n",
      "epoch 908: loss 0.056356\n",
      "epoch 909: loss 0.056111\n",
      "epoch 910: loss 0.056058\n",
      "epoch 911: loss 0.056163\n",
      "epoch 912: loss 0.056148\n",
      "epoch 913: loss 0.055982\n",
      "epoch 914: loss 0.055881\n",
      "epoch 915: loss 0.055920\n",
      "epoch 916: loss 0.055938\n",
      "epoch 917: loss 0.055840\n",
      "epoch 918: loss 0.055733\n",
      "epoch 919: loss 0.055718\n",
      "epoch 920: loss 0.055735\n",
      "epoch 921: loss 0.055686\n",
      "epoch 922: loss 0.055595\n",
      "epoch 923: loss 0.055547\n",
      "epoch 924: loss 0.055545\n",
      "epoch 925: loss 0.055523\n",
      "epoch 926: loss 0.055458\n",
      "epoch 927: loss 0.055396\n",
      "epoch 928: loss 0.055371\n",
      "epoch 929: loss 0.055355\n",
      "epoch 930: loss 0.055313\n",
      "epoch 931: loss 0.055255\n",
      "epoch 932: loss 0.055213\n",
      "epoch 933: loss 0.055189\n",
      "epoch 934: loss 0.055161\n",
      "epoch 935: loss 0.055116\n",
      "epoch 936: loss 0.055068\n",
      "epoch 937: loss 0.055033\n",
      "epoch 938: loss 0.055005\n",
      "epoch 939: loss 0.054971\n",
      "epoch 940: loss 0.054928\n",
      "epoch 941: loss 0.054886\n",
      "epoch 942: loss 0.054853\n",
      "epoch 943: loss 0.054822\n",
      "epoch 944: loss 0.054786\n",
      "epoch 945: loss 0.054745\n",
      "epoch 946: loss 0.054707\n",
      "epoch 947: loss 0.054673\n",
      "epoch 948: loss 0.054640\n",
      "epoch 949: loss 0.054604\n",
      "epoch 950: loss 0.054566\n",
      "epoch 951: loss 0.054529\n",
      "epoch 952: loss 0.054494\n",
      "epoch 953: loss 0.054461\n",
      "epoch 954: loss 0.054425\n",
      "epoch 955: loss 0.054389\n",
      "epoch 956: loss 0.054352\n",
      "epoch 957: loss 0.054317\n",
      "epoch 958: loss 0.054283\n",
      "epoch 959: loss 0.054248\n",
      "epoch 960: loss 0.054212\n",
      "epoch 961: loss 0.054176\n",
      "epoch 962: loss 0.054141\n",
      "epoch 963: loss 0.054107\n",
      "epoch 964: loss 0.054072\n",
      "epoch 965: loss 0.054037\n",
      "epoch 966: loss 0.054002\n",
      "epoch 967: loss 0.053967\n",
      "epoch 968: loss 0.053932\n",
      "epoch 969: loss 0.053898\n",
      "epoch 970: loss 0.053863\n",
      "epoch 971: loss 0.053828\n",
      "epoch 972: loss 0.053794\n",
      "epoch 973: loss 0.053759\n",
      "epoch 974: loss 0.053725\n",
      "epoch 975: loss 0.053690\n",
      "epoch 976: loss 0.053656\n",
      "epoch 977: loss 0.053621\n",
      "epoch 978: loss 0.053587\n",
      "epoch 979: loss 0.053552\n",
      "epoch 980: loss 0.053518\n",
      "epoch 981: loss 0.053484\n",
      "epoch 982: loss 0.053450\n",
      "epoch 983: loss 0.053415\n",
      "epoch 984: loss 0.053381\n",
      "epoch 985: loss 0.053347\n",
      "epoch 986: loss 0.053313\n",
      "epoch 987: loss 0.053279\n",
      "epoch 988: loss 0.053245\n",
      "epoch 989: loss 0.053211\n",
      "epoch 990: loss 0.053177\n",
      "epoch 991: loss 0.053143\n",
      "epoch 992: loss 0.053109\n",
      "epoch 993: loss 0.053075\n",
      "epoch 994: loss 0.053041\n",
      "epoch 995: loss 0.053007\n",
      "epoch 996: loss 0.052974\n",
      "epoch 997: loss 0.052940\n",
      "epoch 998: loss 0.052906\n",
      "epoch 999: loss 0.052872\n",
      "epoch 1000: loss 0.052839\n",
      "epoch 1000: loss 0.052805\n",
      "epoch 1001: loss 0.052732\n",
      "epoch 1002: loss 0.052191\n",
      "epoch 1003: loss 0.049943\n",
      "epoch 1004: loss 0.047543\n",
      "epoch 1005: loss 0.044766\n",
      "epoch 1006: loss 0.041692\n",
      "epoch 1007: loss 0.038124\n",
      "epoch 1008: loss 0.036028\n",
      "epoch 1009: loss 0.033529\n",
      "epoch 1010: loss 0.031477\n",
      "epoch 1011: loss 0.029298\n",
      "epoch 1012: loss 0.027558\n",
      "epoch 1013: loss 0.026365\n",
      "epoch 1014: loss 0.025300\n",
      "epoch 1015: loss 0.024350\n",
      "epoch 1016: loss 0.023541\n",
      "epoch 1017: loss 0.022430\n",
      "epoch 1018: loss 0.021542\n",
      "epoch 1019: loss 0.020401\n",
      "epoch 1020: loss 0.019596\n",
      "epoch 1021: loss 0.019124\n",
      "epoch 1022: loss 0.018718\n",
      "epoch 1023: loss 0.018293\n",
      "epoch 1024: loss 0.017821\n",
      "epoch 1025: loss 0.017365\n",
      "epoch 1026: loss 0.016980\n",
      "epoch 1027: loss 0.016613\n",
      "epoch 1028: loss 0.016159\n",
      "epoch 1029: loss 0.015806\n",
      "epoch 1030: loss 0.015443\n",
      "epoch 1031: loss 0.015023\n",
      "epoch 1032: loss 0.014733\n",
      "epoch 1033: loss 0.014485\n",
      "epoch 1034: loss 0.014298\n",
      "epoch 1035: loss 0.014059\n",
      "epoch 1036: loss 0.013674\n",
      "epoch 1037: loss 0.013376\n",
      "epoch 1038: loss 0.013158\n",
      "epoch 1039: loss 0.012828\n",
      "epoch 1040: loss 0.012497\n",
      "epoch 1041: loss 0.012159\n",
      "epoch 1042: loss 0.011838\n",
      "epoch 1043: loss 0.011454\n",
      "epoch 1044: loss 0.010982\n",
      "epoch 1045: loss 0.010353\n",
      "epoch 1046: loss 0.009898\n",
      "epoch 1047: loss 0.009620\n",
      "epoch 1048: loss 0.009325\n",
      "epoch 1049: loss 0.009082\n",
      "epoch 1050: loss 0.008722\n",
      "epoch 1051: loss 0.008477\n",
      "epoch 1052: loss 0.008204\n",
      "epoch 1053: loss 0.007954\n",
      "epoch 1054: loss 0.007787\n",
      "epoch 1055: loss 0.007634\n",
      "epoch 1056: loss 0.007469\n",
      "epoch 1057: loss 0.007271\n",
      "epoch 1058: loss 0.007089\n",
      "epoch 1059: loss 0.006931\n",
      "epoch 1060: loss 0.006735\n",
      "epoch 1061: loss 0.006546\n",
      "epoch 1062: loss 0.006321\n",
      "epoch 1063: loss 0.006070\n",
      "epoch 1064: loss 0.005805\n",
      "epoch 1065: loss 0.005559\n",
      "epoch 1066: loss 0.005325\n",
      "epoch 1067: loss 0.005190\n",
      "epoch 1068: loss 0.005043\n",
      "epoch 1069: loss 0.004876\n",
      "epoch 1070: loss 0.004731\n",
      "epoch 1071: loss 0.004562\n",
      "epoch 1072: loss 0.005427\n",
      "epoch 1073: loss 0.004355\n",
      "epoch 1074: loss 0.004280\n",
      "epoch 1075: loss 0.004144\n",
      "epoch 1076: loss 0.003963\n",
      "epoch 1077: loss 0.003825\n",
      "epoch 1078: loss 0.003699\n",
      "epoch 1079: loss 0.003513\n",
      "epoch 1080: loss 0.003426\n",
      "epoch 1081: loss 0.003356\n",
      "epoch 1082: loss 0.003237\n",
      "epoch 1083: loss 0.003143\n",
      "epoch 1084: loss 0.003074\n",
      "epoch 1085: loss 0.002996\n",
      "epoch 1086: loss 0.002909\n",
      "epoch 1087: loss 0.002805\n",
      "epoch 1088: loss 0.002714\n",
      "epoch 1089: loss 0.002660\n",
      "epoch 1090: loss 0.002606\n",
      "epoch 1091: loss 0.002555\n",
      "epoch 1092: loss 0.002483\n",
      "epoch 1093: loss 0.002386\n",
      "epoch 1094: loss 0.002322\n",
      "epoch 1095: loss 0.002249\n",
      "epoch 1096: loss 0.002203\n",
      "epoch 1097: loss 0.002156\n",
      "epoch 1098: loss 0.002107\n",
      "epoch 1099: loss 0.002051\n",
      "epoch 1100: loss 0.001997\n",
      "epoch 1101: loss 0.001977\n",
      "epoch 1102: loss 0.001933\n",
      "epoch 1103: loss 0.001885\n",
      "epoch 1104: loss 0.001834\n",
      "epoch 1105: loss 0.001781\n",
      "epoch 1106: loss 0.001750\n",
      "epoch 1107: loss 0.001724\n",
      "epoch 1108: loss 0.001698\n",
      "epoch 1109: loss 0.001675\n",
      "epoch 1110: loss 0.001650\n",
      "epoch 1111: loss 0.001625\n",
      "epoch 1112: loss 0.001601\n",
      "epoch 1113: loss 0.001561\n",
      "epoch 1114: loss 0.001526\n",
      "epoch 1115: loss 0.001499\n",
      "epoch 1116: loss 0.001472\n",
      "epoch 1117: loss 0.001435\n",
      "epoch 1118: loss 0.001412\n",
      "epoch 1119: loss 0.001380\n",
      "epoch 1120: loss 0.001363\n",
      "epoch 1121: loss 0.001346\n",
      "epoch 1122: loss 0.001324\n",
      "epoch 1123: loss 0.001293\n",
      "epoch 1124: loss 0.001274\n",
      "epoch 1125: loss 0.001262\n",
      "epoch 1126: loss 0.001253\n",
      "epoch 1127: loss 0.001246\n",
      "epoch 1128: loss 0.001229\n",
      "epoch 1129: loss 0.001204\n",
      "epoch 1130: loss 0.001190\n",
      "epoch 1131: loss 0.001178\n",
      "epoch 1132: loss 0.001161\n",
      "epoch 1133: loss 0.001136\n",
      "epoch 1134: loss 0.001123\n",
      "epoch 1135: loss 0.001111\n",
      "epoch 1136: loss 0.001102\n",
      "epoch 1137: loss 0.001095\n",
      "epoch 1138: loss 0.001084\n",
      "epoch 1139: loss 0.001079\n",
      "epoch 1140: loss 0.001072\n",
      "epoch 1141: loss 0.001065\n",
      "epoch 1142: loss 0.001059\n",
      "epoch 1143: loss 0.001050\n",
      "epoch 1144: loss 0.001042\n",
      "epoch 1145: loss 0.001037\n",
      "epoch 1146: loss 0.001032\n",
      "epoch 1147: loss 0.001029\n",
      "epoch 1148: loss 0.001021\n",
      "epoch 1149: loss 0.001014\n",
      "epoch 1150: loss 0.001003\n",
      "Converged\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001                                                           # Learning rate\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# %%\n",
    "epoch = 0\n",
    "epochi = epoch\n",
    "epochs = 1001\n",
    "tic = time.time()\n",
    "for epoch in range(1+epochi, epochs+epochi):\n",
    "    loss = optimizer.step(closure)\n",
    "    loss_value = loss.item() if not isinstance(loss, float) else loss\n",
    "    # Print total loss\n",
    "    print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    "        \n",
    "\n",
    "                                                      # Learning rate\n",
    "optimizer = torch.optim.LBFGS(model.parameters(),lr=0.1,max_iter=20)\n",
    "\n",
    "# %%\n",
    "\n",
    "epochi = epoch\n",
    "\n",
    "epochs = 501\n",
    "tic = time.time()\n",
    "for epoch in range(epochi, epochs+epochi):\n",
    "    loss = optimizer.step(closure)\n",
    "    loss_value = loss.item() if not isinstance(loss, float) else loss\n",
    "    if loss_value<er_c1:\n",
    "        # loss = optimizer.step(closure1)\n",
    "        # if loss_value<er_c2:\n",
    "            print('Converged')\n",
    "            break\n",
    "    # Print total loss\n",
    "    print(f'epoch {epoch}: loss {loss_value:.6f}')\n",
    "toc = time.time()\n",
    "# torch.save(model.state_dict(), 'so_model.pth')\n",
    "# print(f'Total training time: {toc - tic}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(x_min, x_max, ct*nx)                                   \n",
    "t = np.linspace(t_max, t_max, 1)                                     \n",
    "t_grid, x_grid = np.meshgrid(t, x)                               \n",
    "T = t_grid.flatten()[:, None]                                    \n",
    "X = x_grid.flatten()[:, None]                                    \n",
    "x_test = np.hstack((T, X))                                       \n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "u_pred = to_numpy(model(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20fe5a24340>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAviklEQVR4nO3de3yU1b3v8e9MbgMxGQiYCxAhUFBiKpawgwE5PaUQoW6sPe2RahG16EusbgS2Vin7mMbtORx7YdNWiVZFjy+QskWp4qbRtCp3pYRQDaFFQzBcJsQkMAnEBEie80ecSMhtZjIzz1w+79dr/siTNWTleaHzZT1r/X4WwzAMAQAAmMRq9gQAAEBkI4wAAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAEwVbfYE3NHW1qYTJ04oISFBFovF7OkAAAA3GIahxsZGDRs2TFZrz+sfIRFGTpw4ofT0dLOnAQAAvHD06FGNGDGix++HRBhJSEiQ1P7LJCYmmjwbAADgjoaGBqWnp3d8jvckJMKI69FMYmIiYQQAgBDT1xYLNrACAABTEUYAAICpCCMAAMBUhBEAAGAqwggAADAVYQQAAJiKMAIAAExFGAEAAKYKiaJn/tDaZmhPZb1qGpuVnGBTTkaSJHW6lj1ysEo+OxVSY8z++eE6JicjSVFW+iIBgD9EZBgpKnOoYHO5HM7mjmuDBsZIkk43ne+4ZrVIbYZCaozZPz9cx6QmxunWnCs0amg84QQAfMxiGIbR97CvbNu2Tb/85S9VUlIih8OhTZs26eabb+71PVu3btXSpUt14MABDRs2TD/96U+1cOFCt39mQ0OD7Ha7nE5nv8vBF5U5dN/affLolwYukWa3KX9OpmZlpZk9FQAIWu5+fnu8Z+Ts2bOaMGGCnnrqKbfGV1ZW6jvf+Y6mTZum0tJS/exnP9OiRYv02muvefqj+621zVDB5nKCCPqt2tms+9buU1GZw+ypAEDI8/gxzezZszV79my3xz/zzDO64oortGrVKknS+PHjtXfvXv3qV7/S97//fU9/fL/sqazv9GgG8JYhySKpYHO5Zmam8sgGAPrB76dpdu/erby8vE7XbrjhBu3du1fnz5/v9j0tLS1qaGjo9PKFmkaCCHzHkORwNus/ig9pd0WdWttYcwMAb/g9jFRXVyslJaXTtZSUFF24cEG1tbXdvmfFihWy2+0dr/T0dJ/MJTnB5pM/B7jYU+99qluf+0DXP/kuj20AwAsBqTNisXRewnbtmb30usuyZcvkdDo7XkePHvXJPHIykpRmt4kFdfgD+0gAwDt+DyOpqamqrq7udK2mpkbR0dEaMmRIt++Ji4tTYmJip5cvRFktyp+TKUkEEvic6yFNweZyHtkAgAf8HkZyc3NVXFzc6do777yjSZMmKSYmxt8/votZWWkqnDdRqfbOj2wGDYzpqC/hcumexFAYY/bPD9cx7nLtI9lTWe/V+wEgEnl8mubMmTP69NNPO76urKzU/v37lZSUpCuuuELLli3T8ePH9fLLL0uSFi5cqKeeekpLly7VPffco927d+uFF17Q+vXrffdbeGhWVppmZqaGZeVSs39+OI45Utuk9XuqVN3g/gZoNksDgPs8Lnr2/vvv61vf+laX63fccYdeeukl3XnnnTpy5Ijef//9ju9t3bpVS5Ys6Sh69sgjj5hW9Azwhqt9wM5PP9dT71X0OX79Pdcpd0z3jyEBIFK4+/ntcRgxA2EEwaK1zdD1T76ramdzt8XzLJJS7TbteGQ6tUcARDy/VWAFIpk7m6Dz52QSRADAA4QRwEM9bYKOjbKqcN5E+tUAgIcismsv0F8Xb4L+x8kGPb65XOda21R75pze2H+czr4A4AH2jAA+MPfZ3frwkuO8dPYFEOnYMwIESFGZo0sQkajICgDuIowA/dDaZqhgc3m336MiKwC4hzAC9MOeyno5nD0XOKMiKwD0jTAC9IO7lVapyAoAPSOMAP2QnGDre5AH4wAgEhFGgH7IyUhSmt3WYwE0i9pP1bj63QAAuiKMAP1ARVYA6D/CCNBPPVVkjYmyUJEVANxABVbABy6uyPpJTaMee+OAzrca+vqIQWZPDQCCHisjgI9EWS3KHTNE83NHdewR+dPHFDwDgL4QRgA/+Odr2h/NvPURYQQA+kIYAfxgVlaqLBZp/9HTOnaqyezpAEBQI4wAfpCcYNPkLx/VPPN+hd7Yf1y7K+ooCw8A3WADK+Ano4bG64PD9Vr7YZXWflgliU6+ANAdVkYAPygqc+gPe452uU4nXwDoijAC+BidfAHAM4QRwMfo5AsAniGMAD5GJ18A8AxhBPAxOvkCgGcII4CP0ckXADxDGAF8rLdOvq6v6eQLAF8hjAB+0FMn30EDY+jkCwCXoOgZ4CcXd/J9Zuun2nqoVt8en0wQAYBLEEYAP3J18jUMQ1sP1eq9v3+u1jaDRzQAcBEe0wAB8E8ZSUqwRavu7DntP3ra7OkAQFAhjAABEBNl1beuTJYk/fngSZNnAwDBhTACBMiMzBRJ0pv7j9PFFwAuwp4RIEDOt7ZJko6fbtaDf9gviS6+ACCxMgIERFGZQw/959+6XKeLLwAQRgC/c3Xx7e6BDF18AYAwAvgdXXwBoHeEEcDP6OILAL0jjAB+RhdfAOgdYQTwM7r4AkDvCCOAn/XWxdeFLr4AIhlhBAiAnrr4DoyNoosvgIhH0TMgQC7u4vuXgyf1/I5KJcXH6IarU82eGgCYipURIIBcXXyX5o1TbLRVx04169DJM2ZPCwBMRRgBTDAwNlpTxwyRROM8ACCMACZxNc77C2EEQIQjjAAm+fZV7WGk9Ohp1Z5pMXk2AGAewghgklS7TVnDE2UY0nPbDuuN/ce1u6KOHjUAIg6naQATjRoSr7LjDXp22+GOa2l2m/LnZHLcF0DEYGUEMElRmUNvfeTocr3a2az71u5TUVnX7wFAOCKMACZobTNUsLm82++5HtIUbC7nkQ2AiEAYAUywp7JeDmfPXXoNSQ5ns/ZU1gduUgBgEsIIYIKaxp6DiDfjACCUEUYAEyQn2Poe5ME4AAhlhBHABDkZSUqz23rs4mtR+6manIykQE4LAExBGAFMEGW1KH9OpiR1CSSur/PnZCrK2lNcAYDwQRgBTDIrK02F8yYq1d75UczlCXEqnDeROiMAIgZFzwATzcpK08zMVO2prNf/euNjfVpzVndPyyCIAIgorIwAJouyWpQ7ZohuzRkpSfrLwRqTZwQAgUUYAYJE3pddfP96pF6nzp4zeTYAEDiEESBIpCcN1FWpCWozpHf/zuoIgMhBGAGCiGt1pLj8pMkzAYDAIYwAQWRmZqok6b1/1GhjyVHtrqijPw2AsMdpGiCIHDvVJKtFarnQpode/UhSe/Gz/DmZnLABELZYGQGCRFGZQz9Zt0+XLoRUO5t139p9KipzmDMxAPAzwggQBFrbDBVsLld3D2Rc1wo2l/PIBkBYIowAQWBPZb0czp479BqSHM5m7amsD9ykACBACCNAEKhp7DmIeDMOAEKJV2Fk9erVysjIkM1mU3Z2trZv397r+HXr1mnChAkaOHCg0tLSdNddd6murs6rCQPhKDnB1vcgD8YBQCjxOIxs2LBBixcv1vLly1VaWqpp06Zp9uzZqqqq6nb8jh07NH/+fC1YsEAHDhzQq6++qr/+9a+6++67+z15IFzkZCQpzW7r0sHXxaL2UzU5GUmBnBYABITHYWTlypVasGCB7r77bo0fP16rVq1Senq6CgsLux3/wQcfaNSoUVq0aJEyMjJ0/fXX695779XevXv7PXkgXERZLcqfkylJPQaS/DmZirL29F0ACF0ehZFz586ppKREeXl5na7n5eVp165d3b5nypQpOnbsmLZs2SLDMHTy5Elt3LhRN954Y48/p6WlRQ0NDZ1eQLiblZWmwnkTlWrv/Cgm2mpR4byJ1BkBELY8CiO1tbVqbW1VSkpKp+spKSmqrq7u9j1TpkzRunXrNHfuXMXGxio1NVWDBg3S7373ux5/zooVK2S32zte6enpnkwTCFmzstK045HpWn/Pdfo/38uS1SJdaDOUmWY3e2oA4DdebWC1WDovFRuG0eWaS3l5uRYtWqTHHntMJSUlKioqUmVlpRYuXNjjn79s2TI5nc6O19GjR72ZJhCSoqwW5Y4Zotsmj9R1o4dIkt4+0H3YB4Bw4FE5+KFDhyoqKqrLKkhNTU2X1RKXFStWaOrUqXr44YclSddcc43i4+M1bdo0PfHEE0pL67r0HBcXp7i4OE+mBoSlWVmp2lVRp6ID1brnv402ezoA4BcerYzExsYqOztbxcXFna4XFxdrypQp3b6nqalJVmvnHxMVFSWpfUUFQM/yvmycV/LZKdU0UGMEQHjy+DHN0qVL9fzzz2vNmjU6ePCglixZoqqqqo7HLsuWLdP8+fM7xs+ZM0evv/66CgsLdfjwYe3cuVOLFi1STk6Ohg0b5rvfBAhDqXabrk0fJEl6ZmuF3th/nE6+AMKOx117586dq7q6Oj3++ONyOBzKysrSli1bNHLkSEmSw+HoVHPkzjvvVGNjo5566in967/+qwYNGqTp06frySef9N1vAYSxUUPitf/oaa3ZeaTjGp18AYQTixECz0oaGhpkt9vldDqVmJho9nSAgCkqc2jh2n1drru2i3PkF0Awc/fzm940QJBydfLtDp18AYQTwggQpOjkCyBSEEaAIEUnXwCRgjACBCk6+QKIFIQRIEjRyRdApCCMAEGqt06+rq/p5AsgHBBGgCDWUyffQQNjONYLIGx4XPQMQGDNykrTzMxU7ams1/PbD+svf6/RlDFDCCIAwgZhBAgBrk6+A2Kj9Je/1+i9f3yu5vOtssVEmT01AOg3HtMAIWTCCLuGDxqgpnOt2nroc7OnAwA+QRgBQojFYtHsrPZOvls+dpg8GwDwDcIIEGJmf719r8jbB6q1seQoXXwBhDz2jAAh5qSzWVaL1Hy+TQ+9+pEkuvgCCG2sjAAhpKjMoftf2adLF0Kqnc26b+0+FZXx6AZA6CGMACHC1cW3uwcydPEFEMoII0CIoIsvgHBFGAFCBF18AYQrwggQIujiCyBcEUaAEEEXXwDhijAChIjeuvi60MUXQCgijAAhpKcuvpK06ofXUmcEQEii6BkQYi7u4lvT0Kz/veWgahpbZLWwIgIgNLEyAoQgVxff735juH6QPUKS9NZHJ0yeFQB4hzAChLh/vmaYJOm9f3yuxubzJs8GADxHGAFC3Pi0BI2+PF7nLrSp8P0KvbH/OM3zAIQU9owAIc5iseiq1EQd/vysVr9f0XGd5nkAQgUrI0CIKypzaMvHXRvk0TwPQKggjAAhzNU8rzs0zwMQKggjQAijeR6AcEAYAUIYzfMAhAPCCBDCaJ4HIBwQRoAQRvM8AOGAMAKEMJrnAQgHhBEgxPXUPC8hLlqF8yZSZwRA0KPoGRAGLm6e99ZHJ7TuwyoNTYjVDVenmj01AOgTKyNAmHA1z3t09lWKi7aqsrZJB040mD0tAOgTYQQIMwm2GM0YnyJJemP/cZNnAwB9I4wAYeima9s7+W4sOaZNpTTOAxDc2DMChKFzF1plkXSq6byWbNgvicZ5AIIXKyNAmCkqc2jR+v26dB2ExnkAghVhBAgjrsZ53T2QoXEegGBFGAHCCI3zAIQiwggQRmicByAUEUaAMELjPAChiDAChBEa5wEIRYQRIIzQOA9AKCKMAGGmp8Z5klRw09XUGQEQdCh6BoShixvn1TQ264UdlfromFN1Z8+ZPTUA6IKVESBMuRrnfffa4frx1AxJ0qbS4zIMaowACC6sjAARIO/qFMXHRqmqvkn/b9cRDY6PVXJC+0ZW9o8AMBthBIgAA2Oj9fXhg/RBZZ1+vrm84zr9agAEAx7TABGgqMyhDyrrulynXw2AYEAYAcKcq19Nd+hXAyAYEEaAMEe/GgDBjjAChDn61QAIdoQRIMzRrwZAsCOMAGGOfjUAgh1hBAhz9KsBEOwII0AE6KlfzcDYKBXOm0idEQCmougZECEu7lez7dDnKtxaoWirRf/9ymSzpwYgwrEyAkQQV7+ah264Uml2mxqaL+jPB0+aPS0AEY4wAkSgKKtF3584QpL03PbDemP/ce2uqKPwGQBT8JgGiFDJCXGSpL8dderBP+yXRK8aAOZgZQSIQEVlDuW/eaDLdXrVADADYQSIMK5eNd09kKFXDQAzEEaACEOvGgDBxqswsnr1amVkZMhmsyk7O1vbt2/vdXxLS4uWL1+ukSNHKi4uTmPGjNGaNWu8mjCA/qFXDYBg4/EG1g0bNmjx4sVavXq1pk6dqmeffVazZ89WeXm5rrjiim7fc8stt+jkyZN64YUX9LWvfU01NTW6cOFCvycPwHP0qgEQbCyGYXj0YHjy5MmaOHGiCgsLO66NHz9eN998s1asWNFlfFFRkX74wx/q8OHDSkryrvdFQ0OD7Ha7nE6nEhMTvfozALRrbTN0/ZPvqtrZ3O2+EYukVLtNOx6ZTol4AP3i7ue3R49pzp07p5KSEuXl5XW6npeXp127dnX7njfffFOTJk3SL37xCw0fPlzjxo3TQw89pC+++KLHn9PS0qKGhoZOLwC+Qa8aAMHGozBSW1ur1tZWpaSkdLqekpKi6urqbt9z+PBh7dixQ2VlZdq0aZNWrVqljRs36v777+/x56xYsUJ2u73jlZ6e7sk0AfShp141kvSz74ynzgiAgPKq6JnF0vlfTIZhdLnm0tbWJovFonXr1slut0uSVq5cqR/84Ad6+umnNWDAgC7vWbZsmZYuXdrxdUNDA4EE8LGLe9XUNDZr/YdV+qCyXsdONZk9NQARxqMwMnToUEVFRXVZBampqemyWuKSlpam4cOHdwQRqX2PiWEYOnbsmMaOHdvlPXFxcYqLi/NkagC84OpVI0lJ8bH64IU9en3fMU0fn6zTTeeVnGBTTkYSj2wA+JVHj2liY2OVnZ2t4uLiTteLi4s1ZcqUbt8zdepUnThxQmfOnOm4dujQIVmtVo0YMcKLKQPwh6ljhippYKwaW1p1x5q/6sE/7Netz32g6598l4qsAPzK4zojS5cu1fPPP681a9bo4MGDWrJkiaqqqrRw4UJJ7Y9Y5s+f3zH+tttu05AhQ3TXXXepvLxc27Zt08MPP6wf//jH3T6iAWCOd8qrVd90rst1SsQD8DeP94zMnTtXdXV1evzxx+VwOJSVlaUtW7Zo5MiRkiSHw6GqqqqO8ZdddpmKi4v1L//yL5o0aZKGDBmiW265RU888YTvfgsA/eIqEd8dQ+2nbgo2l2tmZiqPbAD4nMd1RsxAnRHAv3ZX1OnW5z7oc9z6e67r2GMCAH3xS50RAOGJEvEAzEQYAUCJeACmIowAUE5GktLsth4rslokpdnbj/kCgK8RRgBQIh6AqQgjACT1XCJ+YGyUCudNpEQ8AL/xqhw8gPB0cYn47Z98rtXvV8gqadrYy82eGoAwRhgB0ImrRPzkjCRt+dihI3VNWvXnQ8oabqc8PAC/IIwA6JbValH2yME6Utek57ZXdlxPs9uUPyeTxzYAfIY9IwC6VVTm0Ov7jne5Tnl4AL5GGAHQhas8fHflmV3XCjaXq7Ut6As4AwgBhBEAXeyprJfD2XO1VUOSw9msPZX1gZsUgLBFGAHQBeXhAQQSYQRAF5SHBxBIhBEAXVAeHkAgEUYAdEF5eACBRBgB0K2eysNL0k9nXUWdEQA+Q9EzAD26uDx8TWOzNpYc0/ZPavX36gazpwYgjBBGAPTKVR5eksZcfpm2f7JDb/3thPIyU3WhrY0S8QD6jTACwG1Zw+36WvJl+rTmjO5/ZV/HdUrEA+gP9owAcFtRmUOf1pzpcp0S8QD6gzACwC2uEvHdoUQ8gP4gjABwCyXiAfgLYQSAWygRD8BfCCMA3EKJeAD+QhgB4BZKxAPwF8IIALdQIh6AvxBGALittxLx/2PicLVcaNPuijpO1ADwiMUwjKD/v0ZDQ4PsdrucTqcSExPNng4Q8VrbjI4S8S/uqNT+Y85O36cIGgDJ/c9vVkYAeMxVIj4u2toliEgUQQPgGcIIAK9QBA2ArxBGAHiFImgAfIUwAsArFEED4CuEEQBeoQgaAF8hjADwCkXQAPgKYQSAV/oqgmZI+uE/peutj05QewRAr6gzAqBfisocKthc3mUzq0VfnaqRqD0CRCJ3P78JIwD67eIiaIc/P6Pf/OXTLmNcqyeF8yYSSIAIQdEzAAHjKoL2z9cM03/uPdbtGGqPAOgJYQSAz1B7BIA3CCMAfIbaIwC8QRgB4DPUHgHgDcIIAJ/pq/aIJA0aEKM2w2DfCIAOhBEAPtNX7RFJOv3Fef3o+Q91/ZPv0tUXgCTCCAAfm5WVpsJ5E5Vq7/1RTLWzWfet3UcgAUAYAeB7s7LStOOR6Vq3YLIGDYjpdgxHfQG4EEYA+EWU1SKr1aLTX5zvcQxHfQFIUrTZEwAQvtw9wvunLx/V5GQkKcra2/ZXAOGIMALAb9w9wvvy7s/08u7P6F8DRCge0wDwG3eO+l6MTa1AZCKMAPAbd476Xsz48vXoax9r56e1bGwFIgRhBIBfuXvU92LUIgEii8UwjKD/p4e7LYgBBK/WNkN7Kuv1pzKHXt79mVvvsah9pWTJjLEaNTReyQk2NrkCIcTdz282sAIIiCirRbljhkiS22HE9S+l//jzJx3X2OQKhB/CCICAcm1qrXY2y5tlWYezWQvX7tOCqaM0IzNVORlJkqQ9lfWqaWxWcoJN2SMHq+SzUx1fB+MYVneAr/CYBkDAFZU5dN/afZLkVSC52KCB7RVeTzd9VVzNapEu3vsabGNY3UGkcPfzmzACwBRFZQ4VbC6Xw+leYbRw4loTKZw3kUCCsObu5zenaQCYwp3+NeGKvjxAZ4QRAKaJslo0dexQ/d/vf10WuVeLJFzQlwf4CmEEgOm8qUUSLtzt3wOEM07TAAgKs7LSNDMzteMUypHaJq368yFJ/d/kGszc7d8DhDPCCICgcXEtEkm6MvWysN3kapGUav/qKDAQyQgjAILWxaslxeXVWrPzSEdV1nCQPyeTeiOACCMAgpxrtSR3zBDlZCR1WSkJthoi7oyJj4vSr//nBI71Al8ijAAIGZfuKwnW6qo9jXnroxNa92GVxiVfRhABLkLRMwAIkKP1TZr2i/cUE2XRR/k3aEBslNlTAvyKomcAEGRGDB6gNLtN51sNlR49ZfZ0gKDhVRhZvXq1MjIyZLPZlJ2dre3bt7v1vp07dyo6OlrXXnutNz8WAEKaxWLRP41qf4zz10rCCODicRjZsGGDFi9erOXLl6u0tFTTpk3T7NmzVVVV1ev7nE6n5s+fr29/+9teTxYAQl3HnpIjdSbPBAgeHoeRlStXasGCBbr77rs1fvx4rVq1Sunp6SosLOz1fffee69uu+025ebmej1ZAAh1rjCy77PTOt/aZvJsgODgURg5d+6cSkpKlJeX1+l6Xl6edu3a1eP7XnzxRVVUVCg/P9+tn9PS0qKGhoZOLwAIB1+7/DINGhijL863quy40+zpAEHBozBSW1ur1tZWpaSkdLqekpKi6urqbt/zySef6NFHH9W6desUHe3eSeIVK1bIbrd3vNLT0z2ZJgAELav1on0jR2iSB0hebmC1WDpXDDQMo8s1SWptbdVtt92mgoICjRs3zu0/f9myZXI6nR2vo0ePejNNAAhKOaO+qkUCwMOiZ0OHDlVUVFSXVZCampouqyWS1NjYqL1796q0tFQPPPCAJKmtrU2GYSg6OlrvvPOOpk+f3uV9cXFxiouL82RqABAyXPtGdlfU6Y+lx5WS2F4sjdLwiFQehZHY2FhlZ2eruLhY3/ve9zquFxcX67vf/W6X8YmJifr44487XVu9erXeffddbdy4URkZGV5OGwBC17FTTbJIOnuuVYs37Jckpdltyp+TSWVWRCSPy8EvXbpUt99+uyZNmqTc3Fz9/ve/V1VVlRYuXCip/RHL8ePH9fLLL8tqtSorK6vT+5OTk2Wz2bpcB4BIUFTm0AOvlHZp9lftbNZ9a/epcN5EAgkijsdhZO7cuaqrq9Pjjz8uh8OhrKwsbdmyRSNHjpQkORyOPmuOAEAkam0zVLC5vNuuw4Yki6SCzeWamZnKIxtEFHrTAECA7K6o063PfdDnuPX3XKfcMUMCMCPAv+hNAwBBpqax2afjgHBBGAGAAElOsPl0HBAuCCMAECA5GUlKs9vU024Qi9pP1biO/gKRgjACAAESZbUof06mJHUJJK6v8+dksnkVEYcwAgABNCsrTYXzJirV3vlRTKrdxrFeRCyPj/YCAPpnVlaaZmam6qVdlfr3tw4qOSFOOx6ZzooIIhYrIwBggiirRTdNGC5J+vxMi863tpk8I8A8hBEAMMnQy2KVaIuWYUif1TWZPR3ANIQRADCJxWLRmOTLJEkVn58xeTaAeQgjAGCi0UO/DCM1hBFELsIIAJhoTHK8JFZGENkIIwBgojGXux7TnDV5JoB5CCMAYCJXGDn8+RmFQN9SwC8IIwBgoiuSBirKatHZc6062dBi9nQAUxBGAMBEsdFWjUwaKIl9I4hchBEAMNnoyznei8hGGAEAk7lO1BxmEysiFGEEAEw2ZigrI4hshBEAMFlHrREKnyFCEUYAwGSuKqwnnM0623LB5NkAgUcYAQCTDY6P1ZD4WElSZS37RhB5CCMAEARGX05ZeEQuwggABAHKwiOSEUYAIAiModYIIhhhBACCQMbQ9iqspZ+d0u6KOrW20acGkYMwAgAmKypzaNnrZZLaT9Tc+twHuv7Jd1VU5jB5ZkBgEEYAwERFZQ7dt3afPj/TuUletbNZ963dRyBBRCCMAIBJWtsMFWwuV3cPZFzXCjaX88gGYY8wAgAm2VNZL4ezucfvG5IczmbtqawP3KQAExBGAMAkNY09BxFvxgGhijACACZJTrD5dBwQqggjAGCSnIwkpdltsvQyJik+RtUNzRz3RVgjjACASaKsFuXPyZSkHgNJ/dnzWrJhP8d9EdYshmEEfdRuaGiQ3W6X0+lUYmKi2dMBAJ8qKnOoYHN5r5tZpfbAYkhaMmOsRg2NV3KCTdkjB6vks1OqaWxWcoJNORlJkto3x7qumT0mytrb2g/Cmbuf34QRAAgCrW2G9lTWq9r5hf79vw6q/uw5t95ntUgXP70ZNDBGknS66XxQjEmz25Q/J1OzstLc+n0QXtz9/OYxDQAEgSirRbljhijVPsDtICJ1/uCX2oPBxeHA7DEUb4M7CCMAEETC7RgvxdvgDsIIAASRcDzGS/E29IUwAgBBxJ3jvqEq3FZ94DuEEQAIIu4c9w1V4bjqA98gjABAkJmVlabCeROVag+PD2+L2k/VuI4CA5eKNnsCAICuZmWlaWZmakcNjyO1TVr150OS1G2X32DlWt3Jn5NJvRH0iDACAEHKddzX5crUy7oURwv2OiOp1BmBGwgjABAiLl0tCYbqqt2NefS1v+n10hOaOT5Fz9yezYoI+kQFVgCAT724s1IFm8t14zVpevq2iWZPByaiAisAwBSDB8ZKkk43uV9JFpGNMAIA8KnB8e1hpP7s+T5GAu0IIwAAn0r6cmXklAc9dhDZCCMAAJ9ynbI51XROIbAtEUGAMAIA8KmkLx/TtFxo0xfnW02eDUIBYQQA4FMDY6MUG9X+8XKqiX0j6BthBADgUxaLRYPjv3xUw74RuIEwAgDwOdfx3nrCCNxAGAEA+JwrjJyi1gjcQBgBAPicaxMrj2ngDsIIAMDnvjreywZW9I0wAgDwuY6VER7TwA2EEQCAzw1iAys8QBgBAPhc0pdHe0/zmAZuIIwAAHyOo73wBGEEAOBzrjBymj0jcANhBADgc64NrPWEEbiBMAIA8DnX0d7m82364hzN8tA7wggAwOcui4tWTJRFEsd70TfCCADA5ywWC5tY4TbCCADAL77axMrxXvTOqzCyevVqZWRkyGazKTs7W9u3b+9x7Ouvv66ZM2fq8ssvV2JionJzc/X22297PWEAQGgY/GWtETaxoi8eh5ENGzZo8eLFWr58uUpLSzVt2jTNnj1bVVVV3Y7ftm2bZs6cqS1btqikpETf+ta3NGfOHJWWlvZ78gCA4NXRuZfHNOiDxTAMw5M3TJ48WRMnTlRhYWHHtfHjx+vmm2/WihUr3Pozrr76as2dO1ePPfaYW+MbGhpkt9vldDqVmJjoyXQBACb52aaP9cqHVVo8Y6wWzxhn9nRgAnc/vz1aGTl37pxKSkqUl5fX6XpeXp527drl1p/R1tamxsZGJSUl9TimpaVFDQ0NnV4AgNCSxMoI3ORRGKmtrVVra6tSUlI6XU9JSVF1dbVbf8avf/1rnT17VrfcckuPY1asWCG73d7xSk9P92SaAIAg4Ko1cooNrOiDVxtYLRZLp68Nw+hyrTvr16/Xz3/+c23YsEHJyck9jlu2bJmcTmfH6+jRo95MEwBgIlcVVuqMoC/RngweOnSooqKiuqyC1NTUdFktudSGDRu0YMECvfrqq5oxY0avY+Pi4hQXF+fJ1AAAQYY6I3CXRysjsbGxys7OVnFxcafrxcXFmjJlSo/vW79+ve6880698soruvHGG72bKQAgpAyOp84I3OPRyogkLV26VLfffrsmTZqk3Nxc/f73v1dVVZUWLlwoqf0Ry/Hjx/Xyyy9Lag8i8+fP129+8xtdd911HasqAwYMkN1u9+GvAgAIJoO/3DPCygj64nEYmTt3rurq6vT444/L4XAoKytLW7Zs0ciRIyVJDoejU82RZ599VhcuXND999+v+++/v+P6HXfcoZdeeqn/vwEAICi5Vka+ON+q5vOtssVEmTwjBCuP64yYgTojABB6DMPQ2OV/0oU2Q7uXTVeafYDZU0KA+aXOCAAA7rJYLBrEJla4gTACAPCbpC/707CJFb0hjAAA/IaVEbiDMAIA8BtXSfjTFD5DLwgjAAC/cZ2oqT/LYxr0jDACAPCbwR39aVgZQc8IIwAAv6E/DdxBGAEA+A0bWOEOwggAwG842gt3EEYAAH5D5164gzACAPAbVxhhzwh6QxgBAPiN62hv07n2ZnlAdwgjAAC/SbRFK8pqkcS+EfSMMAIA8BuLxUKtEfSJMAIA8KtBA9rDyH99dEK7K+rU2maYPCMEm2izJwAACF9FZQ5V1TdJkp56r0JPvVeh1MQ43ZpzhUYNjVdygk05GUmSpD2V9appbFZygk3ZIwer5LNTHV8zJnBjXI/VAokwAgDwi6Iyh+5bu0+XroNUN7ToP/78ScfXgwZ2rUVitUgXL6AwJjBj0uw25c/J1KysNAWSxTCMoF8va2hokN1ul9PpVGJiotnTAQD0obXN0PVPviuHs9nsqcADrjWRwnkTfRJI3P38Zs8IAMDn9lTWE0RCkGt1omBzeUD39hBGAAA+V9NIEAlVhiSHs1l7KusD9jMJIwAAn0tOsJk9BfRTIAMlYQQA4HM5GUlKs9sU+HMZ8JVABkrCCADA56KsFuXPyZQkAkmIsaj9VI3rKHAgEEYAAH4xKytNhfMmKtXOI5tQ4QqO+XMyA1pvhDojAAC/mZWVppmZqR2Fto7UNmn9nipVN3y1HyHYam1E8phUk+qMEEYAAH4VZbUod8yQjq8fmP61TlVAg7EKaSSPMaMCK0XPAACAX1D0DAAAhATCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqQgjAADAVIQRAABgqpAoB+8qEtvQ0GDyTAAAgLtcn9t9FXsPiTDS2NgoSUpPTzd5JgAAwFONjY2y2+09fj8ketO0tbXpxIkTSkhIkMXiuwY+DQ0NSk9P19GjR+l542fc68DgPgcG9zkwuM+B4c/7bBiGGhsbNWzYMFmtPe8MCYmVEavVqhEjRvjtz09MTOQveoBwrwOD+xwY3OfA4D4Hhr/uc28rIi5sYAUAAKYijAAAAFNFdBiJi4tTfn6+4uLizJ5K2ONeBwb3OTC4z4HBfQ6MYLjPIbGBFQAAhK+IXhkBAADmI4wAAABTEUYAAICpCCMAAMBUYR9GVq9erYyMDNlsNmVnZ2v79u29jt+6dauys7Nls9k0evRoPfPMMwGaaWjz5D6//vrrmjlzpi6//HIlJiYqNzdXb7/9dgBnG9o8/TvtsnPnTkVHR+vaa6/17wTDhKf3uaWlRcuXL9fIkSMVFxenMWPGaM2aNQGabejy9D6vW7dOEyZM0MCBA5WWlqa77rpLdXV1AZptaNq2bZvmzJmjYcOGyWKx6I9//GOf7wn4Z6ERxv7whz8YMTExxnPPPWeUl5cbDz74oBEfH2989tln3Y4/fPiwMXDgQOPBBx80ysvLjeeee86IiYkxNm7cGOCZhxZP7/ODDz5oPPnkk8aePXuMQ4cOGcuWLTNiYmKMffv2BXjmocfTe+1y+vRpY/To0UZeXp4xYcKEwEw2hHlzn2+66SZj8uTJRnFxsVFZWWl8+OGHxs6dOwM469Dj6X3evn27YbVajd/85jfG4cOHje3btxtXX321cfPNNwd45qFly5YtxvLly43XXnvNkGRs2rSp1/FmfBaGdRjJyckxFi5c2OnaVVddZTz66KPdjv/pT39qXHXVVZ2u3XvvvcZ1113ntzmGA0/vc3cyMzONgoICX08t7Hh7r+fOnWv827/9m5Gfn08YcYOn9/lPf/qTYbfbjbq6ukBML2x4ep9/+ctfGqNHj+507be//a0xYsQIv80x3LgTRsz4LAzbxzTnzp1TSUmJ8vLyOl3Py8vTrl27un3P7t27u4y/4YYbtHfvXp0/f95vcw1l3tznS7W1tamxsVFJSUn+mGLY8PZev/jii6qoqFB+fr6/pxgWvLnPb775piZNmqRf/OIXGj58uMaNG6eHHnpIX3zxRSCmHJK8uc9TpkzRsWPHtGXLFhmGoZMnT2rjxo268cYbAzHliGHGZ2FINMrzRm1trVpbW5WSktLpekpKiqqrq7t9T3V1dbfjL1y4oNraWqWlpfltvqHKm/t8qV//+tc6e/asbrnlFn9MMWx4c68/+eQTPfroo9q+fbuio8P2P3ef8uY+Hz58WDt27JDNZtOmTZtUW1urn/zkJ6qvr2ffSA+8uc9TpkzRunXrNHfuXDU3N+vChQu66aab9Lvf/S4QU44YZnwWhu3KiIvFYun0tWEYXa71Nb676+jM0/vssn79ev385z/Xhg0blJyc7K/phRV373Vra6tuu+02FRQUaNy4cYGaXtjw5O90W1ubLBaL1q1bp5ycHH3nO9/RypUr9dJLL7E60gdP7nN5ebkWLVqkxx57TCUlJSoqKlJlZaUWLlwYiKlGlEB/FobtP5WGDh2qqKioLgm7pqamS+JzSU1N7XZ8dHS0hgwZ4re5hjJv7rPLhg0btGDBAr366quaMWOGP6cZFjy9142Njdq7d69KS0v1wAMPSGr/0DQMQ9HR0XrnnXc0ffr0gMw9lHjzdzotLU3Dhw/v1Cp9/PjxMgxDx44d09ixY/0651DkzX1esWKFpk6dqocffliSdM011yg+Pl7Tpk3TE088weq1j5jxWRi2KyOxsbHKzs5WcXFxp+vFxcWaMmVKt+/Jzc3tMv6dd97RpEmTFBMT47e5hjJv7rPUviJy55136pVXXuF5r5s8vdeJiYn6+OOPtX///o7XwoULdeWVV2r//v2aPHlyoKYeUrz5Oz116lSdOHFCZ86c6bh26NAhWa1WjRgxwq/zDVXe3OempiZZrZ0/tqKioiR99S939J8pn4V+2xobBFzHxl544QWjvLzcWLx4sREfH28cOXLEMAzDePTRR43bb7+9Y7zrONOSJUuM8vJy44UXXuBorxs8vc+vvPKKER0dbTz99NOGw+HoeJ0+fdqsXyFkeHqvL8VpGvd4ep8bGxuNESNGGD/4wQ+MAwcOGFu3bjXGjh1r3H333Wb9CiHB0/v84osvGtHR0cbq1auNiooKY8eOHcakSZOMnJwcs36FkNDY2GiUlpYapaWlhiRj5cqVRmlpaccR6mD4LAzrMGIYhvH0008bI0eONGJjY42JEycaW7du7fjeHXfcYXzzm9/sNP799983vvGNbxixsbHGqFGjjMLCwgDPODR5cp+/+c1vGpK6vO64447ATzwEefp3+mKEEfd5ep8PHjxozJgxwxgwYIAxYsQIY+nSpUZTU1OAZx16PL3Pv/3tb43MzExjwIABRlpamvGjH/3IOHbsWIBnHVree++9Xv+fGwyfhRbDYG0LAACYJ2z3jAAAgNBAGAEAAKYijAAAAFMRRgAAgKkIIwAAwFSEEQAAYCrCCAAAMBVhBAAAmIowAgAATEUYAQAApiKMAAAAUxFGAACAqf4/UikGlqHh9fYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x,u_pred[:,0], marker='o', label='Circle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
